\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[top=1.5cm, bottom=1.5cm, left=1.5cm, right=1.5cm]{geometry}

\author{Aldo Sayeg Pasos Trejo. Facultad de Ciencias, Universidad Nacional Autónoma de México.}
\date{\today}
\title{Details of the creation of loads-related input tables for SWITCH Mexico Model at University of California, Berkeley}
\begin{document}
\maketitle
\section{Hourly demand table creation and problems}
Inside of the ''Loads/Data Analysis'' directory of the "Switch\_mexico\_data" repository there is a directory for each of the projections of hourly load in Mexico. There is a high, mid and low projection. Each one of this projections is made by the CENACE (''Centro Nacional de Control de Energia'', National Center for energy control). 
\\
\\This projections consist of a table with the hourly demand for every Control Region of the SEN (''Sistema energetico nacional'', nacional energy system) from 2016 to 2030. On the other hand, the SENER has also a table of the porcentual assigment of load for each Node of every Control Region of the SEN. With this information, we were able to make a table consisting of the hourly demand for all the SEN Nodes from 2016 to 2030 in each of the 3 projections. 
\\
\\Due to the amount of data and , colecting all of this in a single cvs file would have been too large for efficient manipulation inside Python. Also, as these tables were made manually due to the fact that the original format was an xlsx file with several sheets, it would have been really long to create a single file with all the data at these point. 
\\
\\As a result of this, we splitted this hourly demand per node table in 15 cvs files contained in the ''RawTables'' folder of each projection directory, one for each year from 2016 to 2030. Each of the files has the name of the year of which it contains data. All of this tables were made manually in Excel, and then saved to the desired format.
\\
\\However, we still had problems with these manually made tables. These problems were related to data parsing, such as missing information and changes in the node number and name:
\begin{itemize}
\item The information regarding the "31-central" node was missing from the raw tables created manually.
\item Parsing problem regarding the existence of an extra node in the system called "20-tamazunchale". We decided to include this node and assigned it a percentage participation factor for its control region.
\item Confusion between the difference between a pair of nodes called "53-mulege" and "54-loreto". This nodes are marked as the same in some goverment documents and as different in some other.  nodes.
\end{itemize}
Due to this, we used a Python script called "Organized tables creation" to read the tables contained in the "RawTables" directory and then make the corrections regarding these problems.
\section{Python scripts}
\subsection{Problem solutions and "Organized table creation"}
The "Organized table creation" Python script of each projection directory creates the final and corrected hourly-load tables, using as a base the manually created tables in the "RawTables" folder. First of all, it reads each one of the tables contained in the "RawTables" folder. The script appends each table, along its corresponding year, into a single Data Frame. After that, it reads the information corresponding to the "31-central" node from the "RawTables/Central.csv" file and attaches it to the Data Frame.
\\
\\ To fix the "20-tamazunchale" problem, we had to assing it an hourly load value. We decided to take 2\% of the total load of its balancing area ("06-noreste") and assign it to it. These 2\% was taken from the "16-monterrey" node, due to the fact that this "16-monterrey" node participates with a little more than 50\% of the load in the "06-noreste" balancing area. 
\\
\\After comunication with Mexico's SENER ("Secretaria de energia", energy departament), they corrected the problem regarding the "53-mulege" and the "54-loreto nodes". They determined that the node "54-loreto" doesn't exist and that its participation factor of its balancing area is distributed proportionally between the other nodes in its balancing area ("09-baja\_california\_sur-la\_paz")
\\
\\Finally, the script exportes a new csv table for each year of the hourly-load after making all of these corrections. These tables are contained inside the "OrganizedTables" folder of each projection directory.
\subsection*{Other scripts}
Also we made a Python script called "Statistical analysis table creation" that creates some desired tables. First of all, the script created a single data frame called "df" that contained the data of every year (a concatenation of the files in the "OrganizedTables" folder). It exported this file to the "OranizedTables" folder as a large csv file called "HourlyLoadPerNode". 
\\
\\The same script creates another table, called "LoadHighlightsPerNode" that calculates, for every node at each month and year from 2016 to 2030, the peak day of the month ("PeakDay" column), the peak day average load ("PeakDayValue" column) and the average of the month ("MonthlyAverage" column). We will procede to explain how were these columns calculated
\subsubsection{"LoadHighlightsPerNode" data details}
For every Node in the columns of the "LoadHighlightsPerNode" and a given row of year and month, the "PeakDay" column corresponds to the day of that month and year for which the average load per hour (the sum of each hour load divided by 24) is maximum compared to the other days. 
\\
\\The "PeakDayValue" column shows this value, the mean of the hourly load in that day. 
\\
\\On the other hand, the "MonthlyAverage" value calculates the daily average of the daily load for every day in that month and year, but excluding the peak day. For example, if at node "Hermosillo" at year 2016 and month 09, the peak day is 03, the Monthly average column will correspond to the sum of the daily load of days 01,02,04,05,...,n (n is the number of days in month 09 at year 2016) divided by n-1 (as we have excluded the peak day, we hace only summed n-1 days, so for the average of these sum we hace to divide by n-1)
\\
\\
\\
The Script called "Graphical Analysis" displays graphics corresponding to this data. The first part displays the average hour load for every day in a year, for each year from 2016 to 2030, in a separate graphic ("figure", in matplotlib's termns) for every node in the SEN.
\\
\\The second part asks you for an input consisting of a Node, a Month and a year. For that informations, it displays two graphics: one consisting of the hourly load for every day in the month and another one of the hourly load for selected days. 
\\
\\This selected days are the following: the "PeakDay" from the "LoadHighlightsPerNode" table, the hourly average median day of the month (the day of the month whose hourly average load is the median compared to all the other days hourly average load), the day whose hourly average is closest to the "MonthlyAverage" value from "LoadHighlightsPerNode" table and a random day
\\
\\
\\The last script called "Switch input creation" creates a table called "la\_hourly\_demand\_*" where * is high, low or medium depending of the projection directory we are working in. These table is saved at the "OrganizedTables" folder of each projection directory. 
\\
\\ This table is used as input data for the SWITCH model, hence the importance of writing it to the database
\end{document}